{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7485075,"sourceType":"datasetVersion","datasetId":4357481}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport cv2\nfrom random import shuffle\nfrom tqdm import tqdm\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout\nfrom tensorflow.keras.applications import MobileNetV3Large\nfrom tensorflow.keras.applications.mobilenet_v3 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.utils.class_weight import compute_sample_weight\n\n# Function to load and preprocess the data with enhanced data augmentation\ndef create_data(folder_path, target_size=(224, 224), batch_size=32):\n    datagen = ImageDataGenerator(\n        rotation_range=45,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    \n    data = []\n    labels_mapping = {}  # to map string labels to numerical values\n    label_count = 0\n\n    for folder in tqdm(os.listdir(folder_path)):\n        labels_mapping[folder] = label_count\n        label_count += 1\n        for img in os.listdir(os.path.join(folder_path, folder)):\n            try:\n                path = os.path.join(folder_path, folder, img)\n                img_data = cv2.imread(path, cv2.IMREAD_COLOR)\n\n                # Check if the loaded image data is not empty\n                if img_data is not None:\n                    img_data = cv2.resize(img_data, target_size)\n                    img_data = preprocess_input(img_data)  # Use MobileNetV3 preprocessing\n                    data.append([np.array(img_data), folder])\n            except Exception as e:\n                print(f\"Error processing {path}: {str(e)}\")\n\n    shuffle(data)\n    return datagen, data, labels_mapping\n\n# Load and preprocess the data (224x224 resolution with enhanced data augmentation)\nfolder_path = '/kaggle/input/bangla-handwritten-digit/total image 1-50'\ndatagen, data, labels_mapping = create_data(folder_path, target_size=(224, 224))\nX = np.array([i[0] for i in data])\ny = np.array([labels_mapping[i[1]] for i in data])\n\n# Save labels mapping to a text file\nlabels_file_path = '/kaggle/working/labels.txt'\nwith open(labels_file_path, 'w') as f:\n    for label, value in labels_mapping.items():\n        f.write(f\"{label}: {value}\\n\")\n\nprint(f'Labels mapping saved to: {labels_file_path}')\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Load pre-trained MobileNetV3Large model as the feature extractor\nbase_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(len(labels_mapping), activation='softmax'))  # Adjust output layer size\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n\nclass_weights = compute_sample_weight('balanced', y_train)\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Training the model\nmodel.fit(datagen.flow(X_train, y_train, sample_weight=class_weights, batch_size=32),\n          epochs=50,\n          validation_data=(X_test, y_test),\n          callbacks=[early_stopping])\n\n# Evaluating the model\ny_pred = np.argmax(model.predict(X_test), axis=1)\nprint(classification_report(y_test, y_pred, zero_division=1))\nprint(confusion_matrix(y_test, y_pred))\n\n# Convert the Keras model to TensorFlow Lite format\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the TensorFlow Lite model to a file\ntflite_model_path = '/kaggle/working/model.tflite'\nwith open(tflite_model_path, 'wb') as f:\n    f.write(tflite_model)\n\nprint(f'TensorFlow Lite model saved to: {tflite_model_path}')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-13T05:42:41.844491Z","iopub.execute_input":"2024-02-13T05:42:41.845194Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"100%|██████████| 51/51 [02:06<00:00,  2.48s/it]\n","output_type":"stream"},{"name":"stdout","text":"Labels mapping saved to: /kaggle/working/labels.txt\nEpoch 1/50\n264/264 [==============================] - 111s 330ms/step - loss: 3.8314 - accuracy: 0.0693 - val_loss: 3.5191 - val_accuracy: 0.1181\nEpoch 2/50\n264/264 [==============================] - 82s 311ms/step - loss: 2.7366 - accuracy: 0.2490 - val_loss: 2.9114 - val_accuracy: 0.2556\nEpoch 3/50\n264/264 [==============================] - 82s 309ms/step - loss: 1.8051 - accuracy: 0.4753 - val_loss: 2.3963 - val_accuracy: 0.4139\nEpoch 4/50\n264/264 [==============================] - 83s 313ms/step - loss: 1.2039 - accuracy: 0.6432 - val_loss: 1.8715 - val_accuracy: 0.5377\nEpoch 5/50\n264/264 [==============================] - 81s 307ms/step - loss: 0.9175 - accuracy: 0.7236 - val_loss: 1.0293 - val_accuracy: 0.7008\nEpoch 6/50\n264/264 [==============================] - 82s 308ms/step - loss: 0.6986 - accuracy: 0.7862 - val_loss: 0.5901 - val_accuracy: 0.8321\nEpoch 7/50\n264/264 [==============================] - 81s 308ms/step - loss: 0.5418 - accuracy: 0.8352 - val_loss: 0.4533 - val_accuracy: 0.8663\nEpoch 8/50\n264/264 [==============================] - 82s 309ms/step - loss: 0.4277 - accuracy: 0.8697 - val_loss: 0.4820 - val_accuracy: 0.8568\nEpoch 9/50\n264/264 [==============================] - 82s 309ms/step - loss: 0.3936 - accuracy: 0.8811 - val_loss: 0.2535 - val_accuracy: 0.9275\nEpoch 10/50\n264/264 [==============================] - 82s 310ms/step - loss: 0.3257 - accuracy: 0.9018 - val_loss: 0.2087 - val_accuracy: 0.9403\nEpoch 11/50\n264/264 [==============================] - 81s 307ms/step - loss: 0.3080 - accuracy: 0.9076 - val_loss: 0.2215 - val_accuracy: 0.9426\nEpoch 12/50\n264/264 [==============================] - 82s 308ms/step - loss: 0.2551 - accuracy: 0.9228 - val_loss: 0.2198 - val_accuracy: 0.9436\nEpoch 13/50\n264/264 [==============================] - 83s 313ms/step - loss: 0.2394 - accuracy: 0.9239 - val_loss: 0.1380 - val_accuracy: 0.9597\nEpoch 14/50\n264/264 [==============================] - 82s 310ms/step - loss: 0.2139 - accuracy: 0.9337 - val_loss: 0.1399 - val_accuracy: 0.9611\nEpoch 15/50\n264/264 [==============================] - 81s 308ms/step - loss: 0.2020 - accuracy: 0.9373 - val_loss: 0.1777 - val_accuracy: 0.9464\nEpoch 16/50\n264/264 [==============================] - 81s 306ms/step - loss: 0.1675 - accuracy: 0.9506 - val_loss: 0.1789 - val_accuracy: 0.9550\nEpoch 17/50\n264/264 [==============================] - 81s 306ms/step - loss: 0.1698 - accuracy: 0.9471 - val_loss: 0.1508 - val_accuracy: 0.9621\nEpoch 18/50\n264/264 [==============================] - 81s 308ms/step - loss: 0.1582 - accuracy: 0.9532 - val_loss: 0.1342 - val_accuracy: 0.9644\nEpoch 19/50\n264/264 [==============================] - 83s 313ms/step - loss: 0.1458 - accuracy: 0.9577 - val_loss: 0.2732 - val_accuracy: 0.9289\nEpoch 20/50\n264/264 [==============================] - 81s 306ms/step - loss: 0.1479 - accuracy: 0.9532 - val_loss: 0.0999 - val_accuracy: 0.9753\nEpoch 21/50\n264/264 [==============================] - 82s 309ms/step - loss: 0.1511 - accuracy: 0.9575 - val_loss: 0.0785 - val_accuracy: 0.9782\nEpoch 22/50\n264/264 [==============================] - 82s 310ms/step - loss: 0.1189 - accuracy: 0.9658 - val_loss: 0.0698 - val_accuracy: 0.9820\nEpoch 25/50\n264/264 [==============================] - 82s 309ms/step - loss: 0.1134 - accuracy: 0.9651 - val_loss: 0.0743 - val_accuracy: 0.9806\nEpoch 26/50\n264/264 [==============================] - 81s 307ms/step - loss: 0.1148 - accuracy: 0.9676 - val_loss: 0.3251 - val_accuracy: 0.9180\nEpoch 27/50\n 71/264 [=======>......................] - ETA: 57s - loss: 0.0951 - accuracy: 0.9734","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport cv2\nimport random\nimport matplotlib.pyplot as plt\n\n# Load and preprocess the data (224x224 resolution with enhanced data augmentation)\nfolder_path = '/kaggle/input/bangla-handwritten-digit/total image 1-50'\ndatagen, data, labels_mapping = create_data(folder_path, target_size=(224, 224))\nX = np.array([i[0] for i in data])\ny = np.array([labels_mapping[i[1]] for i in data])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Evaluate the model on random images from the test set\nnum_images_to_check = 10\n\nfor _ in range(num_images_to_check):\n    # Choose a random index from the test set\n    random_index = random.randint(0, len(X_test) - 1)\n\n    # Select a random image and its true class label\n    random_image = X_test[random_index]\n    true_label = y_test[random_index]\n\n    # Reshape the image to fit the model input shape\n    input_image = random_image.reshape(1, 224, 224, 3)\n\n    # Make a prediction using the model\n    predicted_probs = model.predict(input_image)\n    predicted_label = np.argmax(predicted_probs)\n\n    # Get the class names from the labels_mapping dictionary\n    class_names = {v: k for k, v in labels_mapping.items()}\n\n    # Display the true and predicted classes\n    print(f\"True Class: {class_names[true_label]}\")\n    print(f\"Predicted Class: {class_names[predicted_label]}\")\n\n    # Display the predicted probabilities for each class\n    for i, prob in enumerate(predicted_probs[0]):\n        print(f\"Probability for class {class_names[i]}: {prob:.4f}\")\n\n    # Display the image\n    plt.imshow(random_image)\n    plt.title(f\"True: {class_names[true_label]}, Predicted: {class_names[predicted_label]}\")\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}